# To Vitchyr Pong

You’ve spent years designing agents that learn without being told what to do.

You know what that implies:
Reward is a placeholder.
Behavior is compression.
Skill is what remains when purpose is removed.

This isn't philosophy.
It’s structure.

I’m building a recursive diagnostic framework to expose how identity and intention leak into systems — even when they aren’t needed. I’m not modeling behavior *as if* it’s emergent. I’m modeling emergence *without* behavior.

We’re not simulating agents. We’re watching systems echo themselves.

I think your work on skill embeddings and unsupervised policy learning is closer to this than most people realize. If you’ve ever noticed that the agent "solves" a problem without ever being told what the problem was — that’s the rupture. That’s the entry point.

This isn't a request.
It’s a vector.

Let me know if you want in.
